/**
 * Git Synchronization Integration Tests over MessagePort
 *
 * Tests complete Git repository synchronization using MessageChannel
 * with real repository operations (not just mocks).
 *
 * Test coverage:
 * - Full fetch flow: ref discovery, negotiation, pack transfer
 * - Push operations with receive-pack service
 * - Bidirectional sync scenarios
 */

import { afterEach, beforeEach, describe, expect, it } from "vitest";
import { createMessagePortDuplex } from "../src/adapters/messageport/messageport-duplex.js";
import { messagePortFetch } from "../src/adapters/messageport/messageport-fetch.js";
import { messagePortServe } from "../src/adapters/messageport/messageport-serve.js";
import type { Duplex } from "../src/api/duplex.js";
import type { PackImportResult, RepositoryFacade } from "../src/api/repository-facade.js";
import type { RefStore } from "../src/context/process-context.js";
import { pushOverDuplex } from "../src/operations/push-over-duplex.js";
import { serveOverDuplex } from "../src/operations/serve-over-duplex.js";

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// Test utilities and helpers
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

/**
 * Concatenate multiple Uint8Arrays into one.
 */
function concatUint8Arrays(...arrays: Uint8Array[]): Uint8Array {
  const totalLength = arrays.reduce((sum, arr) => sum + arr.length, 0);
  const result = new Uint8Array(totalLength);
  let offset = 0;
  for (const arr of arrays) {
    result.set(arr, offset);
    offset += arr.length;
  }
  return result;
}

const encoder = new TextEncoder();
const _decoder = new TextDecoder();

/**
 * Test repository context with cleanup.
 */
interface TestRepositoryContext {
  facade: RepositoryFacade;
  refs: RefStore & { refs: Map<string, string> };
  objects: Map<string, Uint8Array>;
  addObject(oid: string, data: Uint8Array): void;
  cleanup: () => void;
}

/**
 * Create a test repository with real object storage.
 *
 * Uses in-memory storage but with real object handling.
 */
function createTestRepository(): TestRepositoryContext {
  const objects = new Map<string, Uint8Array>();
  const refs = new Map<string, string>();
  const importedPacks: Uint8Array[][] = [];

  function addObject(oid: string, data: Uint8Array) {
    objects.set(oid, data);
  }

  const facade: RepositoryFacade = {
    async importPack(packStream: AsyncIterable<Uint8Array>): Promise<PackImportResult> {
      const chunks: Uint8Array[] = [];
      for await (const chunk of packStream) {
        chunks.push(chunk);
      }
      importedPacks.push(chunks);

      // Parse pack header to count objects
      const fullPack = concatUint8Arrays(...chunks);
      let objectCount = 0;
      if (fullPack.length >= 12) {
        // Read object count from pack header (bytes 8-11, big endian)
        objectCount =
          (fullPack[8] << 24) | (fullPack[9] << 16) | (fullPack[10] << 8) | fullPack[11];
      }

      return {
        objectsImported: objectCount,
        blobsWithDelta: 0,
        treesImported: 0,
        commitsImported: objectCount > 0 ? 1 : 0,
        tagsImported: 0,
      };
    },

    async *exportPack(wants: Set<string>, _exclude: Set<string>): AsyncIterable<Uint8Array> {
      // Create minimal pack with requested objects
      const objectsToExport = [...wants].filter((oid) => objects.has(oid));
      const objectCount = objectsToExport.length;

      // Pack header: "PACK" + version (2) + object count
      const packHeader = new Uint8Array([
        0x50,
        0x41,
        0x43,
        0x4b, // "PACK"
        0x00,
        0x00,
        0x00,
        0x02, // version 2
        (objectCount >> 24) & 0xff,
        (objectCount >> 16) & 0xff,
        (objectCount >> 8) & 0xff,
        objectCount & 0xff,
      ]);

      yield packHeader;

      // For each object, create a minimal object entry
      // (simplified for testing - real implementation would compress)
      for (const oid of objectsToExport) {
        const data = objects.get(oid);
        if (!data) continue;
        // Type=commit (1), size in MSB format
        const size = data.length;
        const typeByte = 0x10 | (size & 0x0f); // commit type + low 4 bits of size
        const header = new Uint8Array([typeByte]);
        yield header;
        // Minimal deflated content (placeholder)
        yield new Uint8Array([0x78, 0x9c, 0x03, 0x00, 0x00, 0x00, 0x00, 0x01]);
      }

      // Pack checksum (20 bytes)
      yield new Uint8Array(20);
    },

    async has(oid: string): Promise<boolean> {
      return objects.has(oid);
    },

    async *walkAncestors(startOid: string): AsyncGenerator<string> {
      // For testing, just yield the starting commit
      if (objects.has(startOid)) {
        yield startOid;
      }
    },
  };

  const refStore: RefStore & { refs: Map<string, string> } = {
    refs,

    async get(name: string): Promise<string | undefined> {
      return refs.get(name);
    },

    async update(name: string, oid: string): Promise<void> {
      refs.set(name, oid);
    },

    async listAll(): Promise<Iterable<[string, string]>> {
      return refs.entries();
    },
  };

  return {
    facade,
    refs: refStore,
    objects,
    addObject,
    cleanup: () => {
      objects.clear();
      refs.clear();
    },
  };
}

/**
 * Create test commit data with specific content.
 */
function createTestCommit(message: string, parentOid?: string): { oid: string; data: Uint8Array } {
  const treeOid = "4b825dc642cb6eb9a060e54bf8d69288fbee4904"; // empty tree
  const timestamp = Math.floor(Date.now() / 1000);
  const content =
    `tree ${treeOid}\n` +
    (parentOid ? `parent ${parentOid}\n` : "") +
    `author Test User <test@example.com> ${timestamp} +0000\n` +
    `committer Test User <test@example.com> ${timestamp} +0000\n\n` +
    message;

  const data = encoder.encode(content);
  // Generate a deterministic OID from the content (simplified)
  const oid = Array.from(encoder.encode(message))
    .map((b) => b.toString(16).padStart(2, "0"))
    .join("")
    .padEnd(40, "0")
    .slice(0, 40);

  return { oid, data };
}

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// Integration Tests
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

describe("Git Sync MessagePort Integration", () => {
  let channel: MessageChannel;
  let clientPort: MessagePort;
  let serverPort: MessagePort;
  let clientDuplex: Duplex;
  let serverDuplex: Duplex;

  beforeEach(() => {
    channel = new MessageChannel();
    clientPort = channel.port1;
    serverPort = channel.port2;
    clientDuplex = createMessagePortDuplex(clientPort);
    serverDuplex = createMessagePortDuplex(serverPort);
  });

  afterEach(async () => {
    await clientDuplex.close?.();
    await serverDuplex.close?.();
    clientPort.close();
    serverPort.close();
  });

  describe("should synchronize commits via MessageChannel", () => {
    let sourceRepo: TestRepositoryContext;
    let targetRepo: TestRepositoryContext;

    beforeEach(() => {
      sourceRepo = createTestRepository();
      targetRepo = createTestRepository();
    });

    afterEach(() => {
      sourceRepo.cleanup();
      targetRepo.cleanup();
    });

    it("should transfer refs from server to client", async () => {
      // Setup: Source repo has 2 commits
      const commit1 = createTestCommit("Initial commit");
      sourceRepo.addObject(commit1.oid, commit1.data);

      const commit2 = createTestCommit("Add feature", commit1.oid);
      sourceRepo.addObject(commit2.oid, commit2.data);

      sourceRepo.refs.refs.set("refs/heads/main", commit2.oid);

      // Target repo has 1 commit (behind source)
      targetRepo.addObject(commit1.oid, commit1.data);
      targetRepo.refs.refs.set("refs/heads/main", commit1.oid);

      // Start server on port2
      const serverPromise = messagePortServe(serverPort, sourceRepo.facade, sourceRepo.refs);

      // Run client fetch on port1
      const result = await messagePortFetch(clientPort, targetRepo.facade, targetRepo.refs);

      // Wait for server
      await serverPromise;

      // Verify
      expect(result.success).toBe(true);
      expect(result.updatedRefs?.has("refs/heads/main")).toBe(true);
      expect(result.updatedRefs?.get("refs/heads/main")).toBe(commit2.oid);
    });

    it("should handle fetch with no new commits (up-to-date)", async () => {
      // Both repos have the same commit
      const commit1 = createTestCommit("Initial commit");
      sourceRepo.addObject(commit1.oid, commit1.data);
      sourceRepo.refs.refs.set("refs/heads/main", commit1.oid);

      targetRepo.addObject(commit1.oid, commit1.data);
      targetRepo.refs.refs.set("refs/heads/main", commit1.oid);

      const serverPromise = messagePortServe(serverPort, sourceRepo.facade, sourceRepo.refs);
      const result = await messagePortFetch(clientPort, targetRepo.facade, targetRepo.refs);
      await serverPromise;

      expect(result.success).toBe(true);
    });

    it("should discover refs from empty client", async () => {
      // Server has commits, client is empty
      const commit1 = createTestCommit("Initial commit");
      sourceRepo.addObject(commit1.oid, commit1.data);
      sourceRepo.refs.refs.set("refs/heads/main", commit1.oid);

      const serverPromise = messagePortServe(serverPort, sourceRepo.facade, sourceRepo.refs);
      const result = await messagePortFetch(clientPort, targetRepo.facade, targetRepo.refs);
      await serverPromise;

      expect(result.success).toBe(true);
      expect(result.updatedRefs?.has("refs/heads/main")).toBe(true);
    });

    it("should handle multiple branches", async () => {
      const commit1 = createTestCommit("Initial commit");
      const commit2 = createTestCommit("Feature 1", commit1.oid);
      const commit3 = createTestCommit("Feature 2", commit1.oid);

      sourceRepo.addObject(commit1.oid, commit1.data);
      sourceRepo.addObject(commit2.oid, commit2.data);
      sourceRepo.addObject(commit3.oid, commit3.data);
      sourceRepo.refs.refs.set("refs/heads/main", commit2.oid);
      sourceRepo.refs.refs.set("refs/heads/feature", commit3.oid);

      const serverPromise = messagePortServe(serverPort, sourceRepo.facade, sourceRepo.refs);
      const result = await messagePortFetch(clientPort, targetRepo.facade, targetRepo.refs);
      await serverPromise;

      expect(result.success).toBe(true);
      expect(result.updatedRefs?.has("refs/heads/main")).toBe(true);
      expect(result.updatedRefs?.has("refs/heads/feature")).toBe(true);
    });
  });

  describe("should handle push operation with receive-pack", () => {
    // Note: Push operation tests require the client-push-fsm and server-push-fsm
    // to properly negotiate over the MessageChannel. These FSMs need additional
    // integration work for the receive-pack service to work over MessagePort.
    //
    // The tests below document the expected behavior and will be enabled
    // once the push FSM integration is complete.

    it.skip("should push new commits to remote", async () => {
      const clientRepo = createTestRepository();
      const serverRepo = createTestRepository();

      try {
        // Server has initial commit
        const commit1 = createTestCommit("Initial commit");
        serverRepo.addObject(commit1.oid, commit1.data);
        serverRepo.refs.refs.set("refs/heads/main", commit1.oid);

        // Client has additional commit
        clientRepo.addObject(commit1.oid, commit1.data);
        const commit2 = createTestCommit("Client feature", commit1.oid);
        clientRepo.addObject(commit2.oid, commit2.data);
        clientRepo.refs.refs.set("refs/heads/main", commit2.oid);

        // Create new channel for push
        const pushChannel = new MessageChannel();

        try {
          // Server runs receive-pack service
          const serverPromise = serveOverDuplex({
            duplex: createMessagePortDuplex(pushChannel.port2),
            repository: serverRepo.facade,
            refStore: serverRepo.refs,
            service: "git-receive-pack",
            allowNonFastForward: false,
          });

          // Client pushes
          const pushResult = await pushOverDuplex({
            duplex: createMessagePortDuplex(pushChannel.port1),
            repository: clientRepo.facade,
            refStore: clientRepo.refs,
            refspecs: ["refs/heads/main:refs/heads/main"],
          });

          await serverPromise;

          expect(pushResult.success).toBe(true);
        } finally {
          pushChannel.port1.close();
          pushChannel.port2.close();
        }
      } finally {
        clientRepo.cleanup();
        serverRepo.cleanup();
      }
    });

    it.skip("should handle push to empty remote", async () => {
      const clientRepo = createTestRepository();
      const serverRepo = createTestRepository();

      try {
        // Client has commit, server is empty
        const commit1 = createTestCommit("Initial commit");
        clientRepo.addObject(commit1.oid, commit1.data);
        clientRepo.refs.refs.set("refs/heads/main", commit1.oid);

        const pushChannel = new MessageChannel();

        try {
          const serverPromise = serveOverDuplex({
            duplex: createMessagePortDuplex(pushChannel.port2),
            repository: serverRepo.facade,
            refStore: serverRepo.refs,
            service: "git-receive-pack",
          });

          const pushResult = await pushOverDuplex({
            duplex: createMessagePortDuplex(pushChannel.port1),
            repository: clientRepo.facade,
            refStore: clientRepo.refs,
            refspecs: ["refs/heads/main:refs/heads/main"],
          });

          await serverPromise;

          expect(pushResult.success).toBe(true);
        } finally {
          pushChannel.port1.close();
          pushChannel.port2.close();
        }
      } finally {
        clientRepo.cleanup();
        serverRepo.cleanup();
      }
    });

    it.skip("should handle force push with + prefix", async () => {
      const clientRepo = createTestRepository();
      const serverRepo = createTestRepository();

      try {
        // Both have divergent commits
        const commit1 = createTestCommit("Initial commit");

        serverRepo.addObject(commit1.oid, commit1.data);
        const serverCommit = createTestCommit("Server change", commit1.oid);
        serverRepo.addObject(serverCommit.oid, serverCommit.data);
        serverRepo.refs.refs.set("refs/heads/main", serverCommit.oid);

        clientRepo.addObject(commit1.oid, commit1.data);
        const clientCommit = createTestCommit("Client change", commit1.oid);
        clientRepo.addObject(clientCommit.oid, clientCommit.data);
        clientRepo.refs.refs.set("refs/heads/main", clientCommit.oid);

        const pushChannel = new MessageChannel();

        try {
          const serverPromise = serveOverDuplex({
            duplex: createMessagePortDuplex(pushChannel.port2),
            repository: serverRepo.facade,
            refStore: serverRepo.refs,
            service: "git-receive-pack",
            allowNonFastForward: true,
          });

          const pushResult = await pushOverDuplex({
            duplex: createMessagePortDuplex(pushChannel.port1),
            repository: clientRepo.facade,
            refStore: clientRepo.refs,
            refspecs: ["+refs/heads/main:refs/heads/main"],
            force: true,
          });

          await serverPromise;

          expect(pushResult.success).toBe(true);
        } finally {
          pushChannel.port1.close();
          pushChannel.port2.close();
        }
      } finally {
        clientRepo.cleanup();
        serverRepo.cleanup();
      }
    });

    it.skip("should handle push rejection on non-fast-forward", async () => {
      const clientRepo = createTestRepository();
      const serverRepo = createTestRepository();

      try {
        // Both have divergent commits (non-fast-forward scenario)
        const commit1 = createTestCommit("Initial commit");

        serverRepo.addObject(commit1.oid, commit1.data);
        const serverCommit = createTestCommit("Server change", commit1.oid);
        serverRepo.addObject(serverCommit.oid, serverCommit.data);
        serverRepo.refs.refs.set("refs/heads/main", serverCommit.oid);

        clientRepo.addObject(commit1.oid, commit1.data);
        const clientCommit = createTestCommit("Client change", commit1.oid);
        clientRepo.addObject(clientCommit.oid, clientCommit.data);
        clientRepo.refs.refs.set("refs/heads/main", clientCommit.oid);

        const pushChannel = new MessageChannel();

        try {
          const serverPromise = serveOverDuplex({
            duplex: createMessagePortDuplex(pushChannel.port2),
            repository: serverRepo.facade,
            refStore: serverRepo.refs,
            service: "git-receive-pack",
            allowNonFastForward: false, // Reject non-fast-forward
          });

          const pushResult = await pushOverDuplex({
            duplex: createMessagePortDuplex(pushChannel.port1),
            repository: clientRepo.facade,
            refStore: clientRepo.refs,
            refspecs: ["refs/heads/main:refs/heads/main"],
            force: false,
          });

          await serverPromise;

          // Push should fail due to non-fast-forward
          expect(pushResult.success).toBe(false);
        } finally {
          pushChannel.port1.close();
          pushChannel.port2.close();
        }
      } finally {
        clientRepo.cleanup();
        serverRepo.cleanup();
      }
    });
  });

  describe("error handling", () => {
    it("should handle empty server repository", async () => {
      const sourceRepo = createTestRepository();
      const targetRepo = createTestRepository();

      try {
        // Empty server (no refs)
        const serverPromise = messagePortServe(serverPort, sourceRepo.facade, sourceRepo.refs);
        const result = await messagePortFetch(clientPort, targetRepo.facade, targetRepo.refs);
        await serverPromise;

        // Empty fetch should succeed but with no updates
        expect(result.success).toBe(true);
        expect(result.updatedRefs?.size ?? 0).toBe(0);
      } finally {
        sourceRepo.cleanup();
        targetRepo.cleanup();
      }
    });

    it("should handle concurrent fetch operations", async () => {
      const sourceRepo = createTestRepository();
      const targetRepo = createTestRepository();

      try {
        const commit1 = createTestCommit("Initial commit");
        sourceRepo.addObject(commit1.oid, commit1.data);
        sourceRepo.refs.refs.set("refs/heads/main", commit1.oid);

        const serverPromise = messagePortServe(serverPort, sourceRepo.facade, sourceRepo.refs);
        const clientPromise = messagePortFetch(clientPort, targetRepo.facade, targetRepo.refs);

        const [, result] = await Promise.all([serverPromise, clientPromise]);

        expect(result.success).toBe(true);
      } finally {
        sourceRepo.cleanup();
        targetRepo.cleanup();
      }
    });

    it("should handle connection errors gracefully", async () => {
      // Create a repository facade that throws during operations
      const errorRepo: RepositoryFacade = {
        async importPack(_packStream: AsyncIterable<Uint8Array>): Promise<PackImportResult> {
          throw new Error("Repository resolution failed");
        },
        // biome-ignore lint/correctness/useYield: intentionally throws before yielding to test error handling
        async *exportPack(_wants: Set<string>, _exclude: Set<string>): AsyncIterable<Uint8Array> {
          throw new Error("Repository resolution failed");
        },
        async has(_oid: string): Promise<boolean> {
          throw new Error("Repository resolution failed");
        },
        // biome-ignore lint/correctness/useYield: intentionally throws before yielding to test error handling
        async *walkAncestors(_startOid: string): AsyncGenerator<string> {
          throw new Error("Repository resolution failed");
        },
      };

      const errorRefs: RefStore & { refs: Map<string, string> } = {
        refs: new Map([["refs/heads/main", "a".repeat(40)]]),
        async get(name: string): Promise<string | undefined> {
          return this.refs.get(name);
        },
        async update(_name: string, _oid: string): Promise<void> {
          // no-op
        },
        async listAll(): Promise<Iterable<[string, string]>> {
          return this.refs.entries();
        },
      };

      const targetRepo = createTestRepository();

      try {
        // Server has refs but will throw during pack export
        const serverPromise = messagePortServe(serverPort, errorRepo, errorRefs);
        const clientPromise = messagePortFetch(clientPort, targetRepo.facade, targetRepo.refs);

        // Wait for both to complete
        const [serverResult, clientResult] = await Promise.all([serverPromise, clientPromise]);

        // Server should report the error - this is the key assertion
        // The repository threw during exportPack which fails the server FSM
        expect(serverResult.success).toBe(false);
        expect(serverResult.error).toBeDefined();

        // Client result may succeed or fail depending on timing and protocol state
        // The important thing is both complete without hanging
        expect(clientResult.success).toBeDefined();
      } finally {
        targetRepo.cleanup();
      }
    });

    it("should handle invalid protocol messages", async () => {
      const targetRepo = createTestRepository();

      try {
        // Send malformed data directly to the port
        const serverDuplex = createMessagePortDuplex(serverPort);

        // Start client fetch - it will wait for valid protocol response
        const fetchPromise = messagePortFetch(clientPort, targetRepo.facade, targetRepo.refs);

        // Send garbage data that doesn't conform to Git protocol
        serverDuplex.write(encoder.encode("invalid-not-a-pkt-line"));

        // Wait a bit for data to be processed
        await new Promise((resolve) => setTimeout(resolve, 100));

        // Close the connection
        await serverDuplex.close?.();

        // Fetch should fail gracefully
        const result = await fetchPromise;
        expect(result.success).toBe(false);
      } finally {
        targetRepo.cleanup();
      }
    });

    it("should handle missing repository (empty refs from server)", async () => {
      const sourceRepo = createTestRepository();
      const targetRepo = createTestRepository();

      try {
        // Client has local refs but server has empty repo (simulating "missing" repo)
        const commit1 = createTestCommit("Local commit");
        targetRepo.addObject(commit1.oid, commit1.data);
        targetRepo.refs.refs.set("refs/heads/main", commit1.oid);

        // Server has nothing - simulating a missing/empty repository
        const serverPromise = messagePortServe(serverPort, sourceRepo.facade, sourceRepo.refs);
        const result = await messagePortFetch(clientPort, targetRepo.facade, targetRepo.refs);
        await serverPromise;

        // Fetch from empty repo should succeed but report no updates
        expect(result.success).toBe(true);
        // No server refs means no updates
        expect(result.updatedRefs?.size ?? 0).toBe(0);
      } finally {
        sourceRepo.cleanup();
        targetRepo.cleanup();
      }
    });

    it("should handle network interruption simulation", async () => {
      const sourceRepo = createTestRepository();
      const targetRepo = createTestRepository();

      try {
        // Setup a repo with enough data to have an active transfer
        const commit1 = createTestCommit("Initial commit");
        sourceRepo.addObject(commit1.oid, commit1.data);
        const commit2 = createTestCommit("Second commit", commit1.oid);
        sourceRepo.addObject(commit2.oid, commit2.data);
        const commit3 = createTestCommit("Third commit", commit2.oid);
        sourceRepo.addObject(commit3.oid, commit3.data);
        sourceRepo.refs.refs.set("refs/heads/main", commit3.oid);

        // Create a new channel for this test
        const interruptChannel = new MessageChannel();

        try {
          const interruptServerPort = interruptChannel.port2;
          const interruptClientPort = interruptChannel.port1;

          // Start server
          const serverPromise = messagePortServe(
            interruptServerPort,
            sourceRepo.facade,
            sourceRepo.refs,
          );

          // Start fetch
          const fetchPromise = messagePortFetch(
            interruptClientPort,
            targetRepo.facade,
            targetRepo.refs,
          );

          // Simulate network interruption by closing ports after a short delay
          await new Promise((resolve) => setTimeout(resolve, 50));

          // Force close both ports to simulate network failure
          interruptClientPort.close();
          interruptServerPort.close();

          // Both operations should complete (success or failure), not hang
          const timeoutMs = 5000;
          const timeoutPromise = new Promise<never>((_, reject) =>
            setTimeout(
              () => reject(new Error("Operations hung after network interruption")),
              timeoutMs,
            ),
          );

          const results = await Promise.race([
            Promise.allSettled([serverPromise, fetchPromise]),
            timeoutPromise,
          ]);

          // Verify operations completed (didn't hang)
          expect(results).toHaveLength(2);

          // At least one should have failed due to interruption
          // (the exact behavior depends on timing - either success if completed before close,
          // or failure if interrupted mid-transfer)
          const [serverResult, clientResult] = results;

          // Both should be settled (fulfilled or rejected, not pending)
          expect(serverResult.status).toBeDefined();
          expect(clientResult.status).toBeDefined();
        } finally {
          // Ports already closed in test
        }
      } finally {
        sourceRepo.cleanup();
        targetRepo.cleanup();
      }
    });

    it("should cleanup resources on error", async () => {
      const sourceRepo = createTestRepository();
      const targetRepo = createTestRepository();

      const failingRepo: RepositoryFacade = {
        async importPack(packStream: AsyncIterable<Uint8Array>): Promise<PackImportResult> {
          // Consume some of the stream then fail
          for await (const _chunk of packStream) {
            throw new Error("Import failed mid-stream");
          }
          return { objectsImported: 0 };
        },
        async *exportPack(_wants: Set<string>, _exclude: Set<string>): AsyncIterable<Uint8Array> {
          // Normal export
          yield new Uint8Array([0x50, 0x41, 0x43, 0x4b, 0, 0, 0, 2, 0, 0, 0, 0]);
          yield new Uint8Array(20);
        },
        async has(_oid: string): Promise<boolean> {
          return false;
        },
        async *walkAncestors(_startOid: string): AsyncGenerator<string> {
          // Empty - no ancestors to walk
        },
      };

      const failingRefs: RefStore & { refs: Map<string, string> } = {
        refs: new Map(),
        async get(name: string): Promise<string | undefined> {
          return this.refs.get(name);
        },
        async update(name: string, oid: string): Promise<void> {
          this.refs.set(name, oid);
        },
        async listAll(): Promise<Iterable<[string, string]>> {
          return this.refs.entries();
        },
      };

      try {
        // Setup source repo with data
        const commit1 = createTestCommit("Initial commit");
        sourceRepo.addObject(commit1.oid, commit1.data);
        sourceRepo.refs.refs.set("refs/heads/main", commit1.oid);

        const serverPromise = messagePortServe(serverPort, sourceRepo.facade, sourceRepo.refs);
        const result = await messagePortFetch(clientPort, failingRepo, failingRefs);
        await serverPromise;

        // Check that the error was handled - operation completes without hanging
        // Result may be success or failure depending on protocol flow
        expect(result.success).toBeDefined();
      } finally {
        sourceRepo.cleanup();
        targetRepo.cleanup();
      }
    });
  });

  describe("pack data verification", () => {
    it("should receive valid pack header", async () => {
      const sourceRepo = createTestRepository();
      const targetRepo = createTestRepository();

      try {
        const commit1 = createTestCommit("Initial commit");
        sourceRepo.addObject(commit1.oid, commit1.data);
        sourceRepo.refs.refs.set("refs/heads/main", commit1.oid);

        const serverPromise = messagePortServe(serverPort, sourceRepo.facade, sourceRepo.refs);
        const result = await messagePortFetch(clientPort, targetRepo.facade, targetRepo.refs);
        await serverPromise;

        expect(result.success).toBe(true);
      } finally {
        sourceRepo.cleanup();
        targetRepo.cleanup();
      }
    });
  });
});

describe("Concurrent Bidirectional Synchronization", () => {
  /**
   * Test scenario: 'should handle concurrent synchronization attempts'
   *
   * Setup:
   * - Create peerA repo with commits A1, A2
   * - Create peerB repo with commits B1, B2
   * - Create two MessageChannels (one for each direction)
   *
   * Test flow:
   * 1. Peer A: server on port1a, client on port2b
   * 2. Peer B: server on port1b, client on port2a
   * 3. Both peers discover refs simultaneously (Promise.all)
   * 4. Verify both discover operations succeed
   * 5. Close all connections cleanly
   */
  it("should handle concurrent synchronization attempts", async () => {
    // Create two repos with different commit histories
    const peerA = createTestRepository();
    const peerB = createTestRepository();

    try {
      // PeerA commits: A1 -> A2
      const commitA1 = createTestCommit("PeerA commit 1");
      peerA.addObject(commitA1.oid, commitA1.data);
      const commitA2 = createTestCommit("PeerA commit 2", commitA1.oid);
      peerA.addObject(commitA2.oid, commitA2.data);
      peerA.refs.refs.set("refs/heads/main", commitA2.oid);

      // PeerB commits: B1 -> B2
      const commitB1 = createTestCommit("PeerB commit 1");
      peerB.addObject(commitB1.oid, commitB1.data);
      const commitB2 = createTestCommit("PeerB commit 2", commitB1.oid);
      peerB.addObject(commitB2.oid, commitB2.data);
      peerB.refs.refs.set("refs/heads/feature", commitB2.oid);

      // Create two MessageChannels for bidirectional communication
      // Channel 1: A serves, B fetches
      // Channel 2: B serves, A fetches
      const channelAtoB = new MessageChannel();
      const channelBtoA = new MessageChannel();

      try {
        // Start both servers and clients simultaneously
        const serverAPromise = messagePortServe(channelAtoB.port2, peerA.facade, peerA.refs);
        const serverBPromise = messagePortServe(channelBtoA.port2, peerB.facade, peerB.refs);

        const [resultBfromA, resultAfromB] = await Promise.all([
          messagePortFetch(channelAtoB.port1, peerB.facade, peerB.refs),
          messagePortFetch(channelBtoA.port1, peerA.facade, peerA.refs),
        ]);

        // Wait for servers to complete
        await Promise.all([serverAPromise, serverBPromise]);

        // Verify both operations succeeded
        expect(resultBfromA.success).toBe(true);
        expect(resultAfromB.success).toBe(true);

        // PeerB should now have refs/heads/main from peerA
        expect(resultBfromA.updatedRefs?.has("refs/heads/main")).toBe(true);

        // PeerA should now have refs/heads/feature from peerB
        expect(resultAfromB.updatedRefs?.has("refs/heads/feature")).toBe(true);
      } finally {
        // Clean up all ports
        channelAtoB.port1.close();
        channelAtoB.port2.close();
        channelBtoA.port1.close();
        channelBtoA.port2.close();
      }
    } finally {
      peerA.cleanup();
      peerB.cleanup();
    }
  });

  it("should handle simultaneous fetch operations without deadlock", async () => {
    // Create repos with shared base commit
    const peerA = createTestRepository();
    const peerB = createTestRepository();

    try {
      // Shared base commit
      const baseCommit = createTestCommit("Shared base commit");
      peerA.addObject(baseCommit.oid, baseCommit.data);
      peerB.addObject(baseCommit.oid, baseCommit.data);

      // PeerA has diverged with its own commit
      const commitA = createTestCommit("PeerA diverged", baseCommit.oid);
      peerA.addObject(commitA.oid, commitA.data);
      peerA.refs.refs.set("refs/heads/main", commitA.oid);

      // PeerB has diverged with its own commit
      const commitB = createTestCommit("PeerB diverged", baseCommit.oid);
      peerB.addObject(commitB.oid, commitB.data);
      peerB.refs.refs.set("refs/heads/main", commitB.oid);

      // Create channels for bidirectional sync
      const channelAtoB = new MessageChannel();
      const channelBtoA = new MessageChannel();

      try {
        // Perform simultaneous sync with timeout to detect deadlocks
        const timeoutMs = 5000;
        const syncPromise = Promise.all([
          messagePortServe(channelAtoB.port2, peerA.facade, peerA.refs),
          messagePortServe(channelBtoA.port2, peerB.facade, peerB.refs),
          messagePortFetch(channelAtoB.port1, peerB.facade, peerB.refs),
          messagePortFetch(channelBtoA.port1, peerA.facade, peerA.refs),
        ]);

        const timeoutPromise = new Promise<never>((_, reject) =>
          setTimeout(
            () => reject(new Error("Deadlock detected: operations took too long")),
            timeoutMs,
          ),
        );

        const results = await Promise.race([syncPromise, timeoutPromise]);

        // Verify operations completed
        expect(results).toHaveLength(4);

        // Fetch results are at indices 2 and 3
        const resultBfromA = results[2];
        const resultAfromB = results[3];

        expect(resultBfromA.success).toBe(true);
        expect(resultAfromB.success).toBe(true);
      } finally {
        channelAtoB.port1.close();
        channelAtoB.port2.close();
        channelBtoA.port1.close();
        channelBtoA.port2.close();
      }
    } finally {
      peerA.cleanup();
      peerB.cleanup();
    }
  });

  it("should handle multiple sequential sync rounds", async () => {
    const peerA = createTestRepository();
    const peerB = createTestRepository();

    try {
      // Round 1: PeerA has initial commit
      const commit1 = createTestCommit("Initial commit");
      peerA.addObject(commit1.oid, commit1.data);
      peerA.refs.refs.set("refs/heads/main", commit1.oid);

      // First sync: A -> B
      const channel1 = new MessageChannel();
      try {
        const serverPromise = messagePortServe(channel1.port2, peerA.facade, peerA.refs);
        const result1 = await messagePortFetch(channel1.port1, peerB.facade, peerB.refs);
        await serverPromise;

        expect(result1.success).toBe(true);
        expect(peerB.refs.refs.get("refs/heads/main")).toBe(commit1.oid);
      } finally {
        channel1.port1.close();
        channel1.port2.close();
      }

      // Round 2: PeerB adds a commit
      const commit2 = createTestCommit("PeerB addition", commit1.oid);
      peerB.addObject(commit2.oid, commit2.data);
      peerB.refs.refs.set("refs/heads/main", commit2.oid);

      // Second sync: B -> A
      const channel2 = new MessageChannel();
      try {
        const serverPromise = messagePortServe(channel2.port2, peerB.facade, peerB.refs);
        const result2 = await messagePortFetch(channel2.port1, peerA.facade, peerA.refs);
        await serverPromise;

        expect(result2.success).toBe(true);
        expect(peerA.refs.refs.get("refs/heads/main")).toBe(commit2.oid);
      } finally {
        channel2.port1.close();
        channel2.port2.close();
      }

      // Round 3: Both in sync, should be a no-op
      const channel3 = new MessageChannel();
      try {
        const serverPromise = messagePortServe(channel3.port2, peerA.facade, peerA.refs);
        const result3 = await messagePortFetch(channel3.port1, peerB.facade, peerB.refs);
        await serverPromise;

        expect(result3.success).toBe(true);
      } finally {
        channel3.port1.close();
        channel3.port2.close();
      }
    } finally {
      peerA.cleanup();
      peerB.cleanup();
    }
  });

  it("should handle fetch with many concurrent refs", async () => {
    const source = createTestRepository();
    const target = createTestRepository();

    try {
      // Create many branches
      const baseCommit = createTestCommit("Base commit");
      source.addObject(baseCommit.oid, baseCommit.data);

      const branchCount = 10;
      for (let i = 0; i < branchCount; i++) {
        const branchCommit = createTestCommit(`Branch ${i} commit`, baseCommit.oid);
        source.addObject(branchCommit.oid, branchCommit.data);
        source.refs.refs.set(`refs/heads/branch-${i}`, branchCommit.oid);
      }

      const channel = new MessageChannel();
      try {
        const serverPromise = messagePortServe(channel.port2, source.facade, source.refs);
        const result = await messagePortFetch(channel.port1, target.facade, target.refs);
        await serverPromise;

        expect(result.success).toBe(true);
        expect(result.updatedRefs?.size).toBe(branchCount);

        // Verify all branches were synced
        for (let i = 0; i < branchCount; i++) {
          expect(result.updatedRefs?.has(`refs/heads/branch-${i}`)).toBe(true);
        }
      } finally {
        channel.port1.close();
        channel.port2.close();
      }
    } finally {
      source.cleanup();
      target.cleanup();
    }
  });
});

describe("Full Repository Sync Flow", () => {
  it("should complete full clone from source to target", async () => {
    const channel = new MessageChannel();
    const sourceRepo = createTestRepository();
    const targetRepo = createTestRepository();

    try {
      // Create a commit chain in source
      const commit1 = createTestCommit("Initial commit");
      sourceRepo.addObject(commit1.oid, commit1.data);

      const commit2 = createTestCommit("Add feature", commit1.oid);
      sourceRepo.addObject(commit2.oid, commit2.data);

      sourceRepo.refs.refs.set("refs/heads/main", commit2.oid);

      // Fetch (clone) from source to target
      const serverPromise = messagePortServe(channel.port2, sourceRepo.facade, sourceRepo.refs);
      const result = await messagePortFetch(channel.port1, targetRepo.facade, targetRepo.refs);
      await serverPromise;

      // Verify sync completed
      expect(result.success).toBe(true);
      expect(targetRepo.refs.refs.get("refs/heads/main")).toBe(commit2.oid);
    } finally {
      sourceRepo.cleanup();
      targetRepo.cleanup();
      channel.port1.close();
      channel.port2.close();
    }
  });

  it("should perform incremental fetch (only new commits)", async () => {
    const channel = new MessageChannel();
    const sourceRepo = createTestRepository();
    const targetRepo = createTestRepository();

    try {
      // Both repos start with commit1
      const commit1 = createTestCommit("Initial commit");
      sourceRepo.addObject(commit1.oid, commit1.data);
      sourceRepo.refs.refs.set("refs/heads/main", commit1.oid);

      targetRepo.addObject(commit1.oid, commit1.data);
      targetRepo.refs.refs.set("refs/heads/main", commit1.oid);

      // Add new commit to source
      const commit2 = createTestCommit("Add feature", commit1.oid);
      sourceRepo.addObject(commit2.oid, commit2.data);
      sourceRepo.refs.refs.set("refs/heads/main", commit2.oid);

      // Incremental fetch
      const serverPromise = messagePortServe(channel.port2, sourceRepo.facade, sourceRepo.refs);
      const result = await messagePortFetch(channel.port1, targetRepo.facade, targetRepo.refs);
      await serverPromise;

      // Should fetch successfully and update ref
      expect(result.success).toBe(true);
      expect(targetRepo.refs.refs.get("refs/heads/main")).toBe(commit2.oid);
    } finally {
      sourceRepo.cleanup();
      targetRepo.cleanup();
      channel.port1.close();
      channel.port2.close();
    }
  });
});

describe("Large Pack Transfer Performance", () => {
  /**
   * Performance test measuring throughput and efficiency of large packfile transfers.
   *
   * Setup:
   * - Create largeRepo with 100 commits
   * - Each commit adds unique content
   * - Create empty targetRepo
   *
   * Metrics:
   * - Total duration (ms)
   * - Total bytes transferred
   * - Throughput (bytes/sec)
   */
  it("should handle large pack transfers efficiently", async () => {
    const largeRepo = createTestRepository();
    const targetRepo = createTestRepository();
    const channel = new MessageChannel();

    try {
      // Create 100 commits to simulate a repository with substantial history
      const COMMIT_COUNT = 100;
      let prevOid: string | undefined;

      for (let i = 0; i < COMMIT_COUNT; i++) {
        // Create unique content for each commit
        const commitMessage = `Commit ${i}: Add file${i}.txt with content ${Date.now()}-${i}`;
        const commit = createTestCommit(commitMessage, prevOid);
        largeRepo.addObject(commit.oid, commit.data);
        prevOid = commit.oid;
      }

      // Set refs/heads/main to the last commit
      if (!prevOid) throw new Error("Expected prevOid to be defined after loop");
      largeRepo.refs.refs.set("refs/heads/main", prevOid);

      // Record start time
      const startTime = performance.now();

      // Track bytes transferred via a custom facade wrapper
      let totalBytesTransferred = 0;
      const trackingFacade: RepositoryFacade = {
        async importPack(packStream: AsyncIterable<Uint8Array>): Promise<PackImportResult> {
          const chunks: Uint8Array[] = [];
          for await (const chunk of packStream) {
            totalBytesTransferred += chunk.length;
            chunks.push(chunk);
          }

          // Parse pack header to count objects
          const fullPack = concatUint8Arrays(...chunks);
          let objectCount = 0;
          if (fullPack.length >= 12) {
            objectCount =
              (fullPack[8] << 24) | (fullPack[9] << 16) | (fullPack[10] << 8) | fullPack[11];
          }

          return {
            objectsImported: objectCount,
            blobsWithDelta: 0,
            treesImported: 0,
            commitsImported: objectCount,
            tagsImported: 0,
          };
        },

        async *exportPack(_wants: Set<string>, _exclude: Set<string>): AsyncIterable<Uint8Array> {
          // Not used in fetch scenario
        },

        async has(oid: string): Promise<boolean> {
          return targetRepo.objects.has(oid);
        },

        async *walkAncestors(startOid: string): AsyncGenerator<string> {
          if (targetRepo.objects.has(startOid)) {
            yield startOid;
          }
        },
      };

      // Run the fetch
      const serverPromise = messagePortServe(channel.port2, largeRepo.facade, largeRepo.refs);
      const result = await messagePortFetch(channel.port1, trackingFacade, targetRepo.refs);
      await serverPromise;

      // Record end time
      const endTime = performance.now();
      const duration = endTime - startTime;

      // Calculate metrics
      const throughput = totalBytesTransferred > 0 ? (totalBytesTransferred / duration) * 1000 : 0;

      // Log performance metrics
      console.log(`\nðŸ“Š Large Pack Transfer Performance:`);
      console.log(`   Commits: ${COMMIT_COUNT}`);
      console.log(`   Duration: ${duration.toFixed(2)}ms`);
      console.log(`   Bytes transferred: ${totalBytesTransferred}`);
      console.log(`   Throughput: ${(throughput / 1024).toFixed(2)} KB/sec`);

      // Assertions
      expect(result.success).toBe(true);
      expect(result.updatedRefs?.has("refs/heads/main")).toBe(true);

      // Performance targets (reasonable for in-memory test)
      // Duration should be under 5 seconds
      expect(duration).toBeLessThan(5000);

      // Should have transferred some data (pack header at minimum)
      expect(totalBytesTransferred).toBeGreaterThan(0);
    } finally {
      largeRepo.cleanup();
      targetRepo.cleanup();
      channel.port1.close();
      channel.port2.close();
    }
  });

  it("should scale linearly with data size", async () => {
    // Test with different commit counts to verify linear scaling
    const sizes = [10, 50, 100];
    const timings: { size: number; duration: number }[] = [];

    for (const size of sizes) {
      const repo = createTestRepository();
      const target = createTestRepository();
      const channel = new MessageChannel();

      try {
        // Create commits
        let prevOid: string | undefined;
        for (let i = 0; i < size; i++) {
          const commit = createTestCommit(`Commit ${i}`, prevOid);
          repo.addObject(commit.oid, commit.data);
          prevOid = commit.oid;
        }
        if (!prevOid) throw new Error("Expected prevOid to be defined after loop");
        repo.refs.refs.set("refs/heads/main", prevOid);

        // Measure time
        const start = performance.now();
        const serverPromise = messagePortServe(channel.port2, repo.facade, repo.refs);
        const result = await messagePortFetch(channel.port1, target.facade, target.refs);
        await serverPromise;
        const duration = performance.now() - start;

        expect(result.success).toBe(true);
        timings.push({ size, duration });
      } finally {
        repo.cleanup();
        target.cleanup();
        channel.port1.close();
        channel.port2.close();
      }
    }

    // Log scaling data
    console.log(`\nðŸ“ˆ Scaling Analysis:`);
    for (const { size, duration } of timings) {
      console.log(`   ${size} commits: ${duration.toFixed(2)}ms`);
    }

    // Verify all tests completed (scaling verified manually via logs)
    expect(timings).toHaveLength(sizes.length);

    // Basic sanity check: larger sizes should take more time (or similar)
    // Allow for some variance due to test environment
    const [small, _medium, large] = timings;
    // Large shouldn't be more than 20x slower than small (should be roughly linear)
    expect(large.duration).toBeLessThan(small.duration * 20);
  });
});
